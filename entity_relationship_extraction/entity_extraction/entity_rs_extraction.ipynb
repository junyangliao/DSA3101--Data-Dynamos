{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity and Relationship Extractions function\n",
    "\n",
    "- argument to be just the original csv file path\n",
    "- if any column names is in the list of column names then we carry out the functions below\n",
    "- one function for entity extraction (exlcuding module info and review csv)\n",
    "- one function for relationship extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to: 07 - Jobs and relevant skillset (linkedin)_extracted.csv\n",
      "             Job Title                                             Skills  \\\n",
      "0     Academic Advisor  Academic Advising, Higher Education, Student D...   \n",
      "1       Academic Tutor  Tutoring, Teaching, Peer Tutoring, Mathematics...   \n",
      "2  Account Coordinator  Account Management, Account Coordination, Publ...   \n",
      "3     Account Director  Account Management, Client Services, Client Re...   \n",
      "4    Account Executive  Software as a Service (SaaS), Account Manageme...   \n",
      "\n",
      "                                      skill_entities  \\\n",
      "0  [(Academic Advising, SKILL), (Higher Education...   \n",
      "1  [(Tutoring, SKILL), (Teaching, SKILL), (Peer T...   \n",
      "2  [(Account Management, SKILL), (Account Coordin...   \n",
      "3  [(Account Management, SKILL), (Client Services...   \n",
      "4  [(Software as a Service (SaaS), SKILL), (Accou...   \n",
      "\n",
      "                   job_entities  \\\n",
      "0     [(Academic Advisor, JOB)]   \n",
      "1       [(Academic Tutor, JOB)]   \n",
      "2  [(Account Coordinator, JOB)]   \n",
      "3     [(Account Director, JOB)]   \n",
      "4    [(Account Executive, JOB)]   \n",
      "\n",
      "                                       relationships  \n",
      "0  [{'from_type': 'JOB', 'from_id': 'Academic Adv...  \n",
      "1  [{'from_type': 'JOB', 'from_id': 'Academic Tut...  \n",
      "2  [{'from_type': 'JOB', 'from_id': 'Account Coor...  \n",
      "3  [{'from_type': 'JOB', 'from_id': 'Account Dire...  \n",
      "4  [{'from_type': 'JOB', 'from_id': 'Account Exec...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast \n",
    "import re\n",
    "import spacy\n",
    "import os \n",
    "from fuzzywuzzy import fuzz, process\n",
    "\n",
    "\n",
    "# Load your spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def extract_entities_rs(csv_file_path): \n",
    "\n",
    "    # Predefined entity columns and their corresponding new column names for entity extraction \n",
    "    target_cols = ['Student_Name', 'Faculties', 'Degree', 'Major', 'Module', 'module_code', 'moduleCode', 'Skills', 'Staff', \n",
    "                   'Modules_Completed', 'department', 'faculty', 'prerequisite', 'preclusion', 'Employee Name', \n",
    "                   'Department', 'Modules Taught', 'Title', 'Job Title', 'Tech Skill', 'university',\n",
    "                   'school', 'degree', 'description', 'message'] \n",
    "\n",
    "    new_entity_cols = {\n",
    "        'Student_Name': ('student_entities', 'STUDENT'),\n",
    "        'Degree': ('degree_entities', 'DEGREE'),\n",
    "        'degree': ('degree_entities', 'DEGREE'),\n",
    "        'Major': ('major_entities', 'MAJOR'),\n",
    "        'Module': ('module_entities', 'MODULE'),\n",
    "        'Modules_Completed': ('module_entities', 'MODULE'),\n",
    "        'module_code': ('module_entities', 'MODULE'),\n",
    "        'moduleCode': ('module_entities', 'MODULE'),\n",
    "        'Modules Taught': ('module_entities', 'MODULE'),\n",
    "        'prerequisite': ('prerequisite_entities', 'PREREQUISITEGROUP'),\n",
    "        'preclusion': ('preclusion_entities', 'PRECLUSIONGROUP'),\n",
    "        'Skills': ('skill_entities', 'SKILL'),\n",
    "        'Tech Skill': ('skill_entities', 'SKILL'),\n",
    "        'Staff': ('staff_entities', 'STAFF'),\n",
    "        'Employee Name': ('staff_entities', 'STAFF'),\n",
    "        'department': ('department_entities', 'DEPARTMENT'), \n",
    "        'Department': ('department_entities', 'DEPARTMENT'), \n",
    "        'Faculties': ('faculty_entities', 'FACULTY'),\n",
    "        'faculty': ('faculty_entities', 'FACULTY'),\n",
    "        'school': ('faculty_entities', 'FACULTY'),\n",
    "        'Title': ('job_entities', 'JOB'),\n",
    "        'Job Title': ('job_entities', 'JOB'),\n",
    "        'university': ('university_entities', 'UNIVERSITY')\n",
    "    }\n",
    "\n",
    "    # Relationship mappings\n",
    "    relationship_mappings = {\n",
    "        ('student_entities', 'faculty_entities'): {\"from_type\": \"STUDENT\", \"to_type\": \"FACULTY\", \"relationship_type\": \"STUDYING_UNDER\"},\n",
    "        ('student_entities', 'major_entities'): {\"from_type\": \"STUDENT\", \"to_type\": \"MAJOR\", \"relationship_type\": \"MAJOR_IN\"},\n",
    "        ('student_entities', 'module_entities'): {\"from_type\": \"STUDENT\", \"to_type\": \"MODULE\", \"relationship_type\": \"COMPLETED\"},\n",
    "        ('module_entities', 'department_entities'): {\"from_type\": \"MODULE\", \"to_type\": \"DEPARTMENT\", \"relationship_type\": \"BELONGS_TO\"},\n",
    "        ('module_entities', 'prerequisite_entities', 'MUST_HAVE_TAKEN_ONE_OF'): {\"from_type\": \"MODULE\", \"to_type\": \"DEPARTMENT\", \"relationship_type\": \"MUST_HAVE_TAKEN_ONE_OF\"},\n",
    "        ('module_entities', 'preclusion_entities', 'MUST_NOT_HAVE_TAKEN_ONE_OF'): {\"from_type\": \"MODULE\", \"to_type\": \"DEPARTMENT\", \"relationship_type\": \"MUST_NOT_HAVE_TAKEN_ONE_OF\"},\n",
    "        ('module_entities', 'prerequisite_entities', 'INCLUDED_AS_PREREQUISITE'): {\"from_type\": \"MODULE\", \"to_type\": \"DEPARTMENT\", \"relationship_type\": \"INCLUDED_AS_PREREQUISITE\"},\n",
    "        ('module_entities', 'preclusion_entities', 'INCLUDED_AS_PRECLUSION'): {\"from_type\": \"MODULE\", \"to_type\": \"DEPARTMENT\", \"relationship_type\": \"INCLUDED_AS_PRECLUSION\"},\n",
    "        ('module_entities', 'semester_entities', 'OFFERED_IN'): {\"from_type\": \"MODULE\", \"to_type\": \"SEMESTER\", \"relationship_type\": \"OFFERED_IN\"},\n",
    "        ('module_entities', 'staff_entities'): {\"from_type\": \"MODULE\", \"to_type\": \"STAFF\", \"relationship_type\": \"TAUGHT_BY\"},\n",
    "        ('staff_entities', 'department_entities'): {\"from_type\": \"STAFF\", \"to_type\": \"DEPARTMENT\", \"relationship_type\": \"EMPLOYED_UNDER\"},\n",
    "        ('department_entities', 'faculty_entities'): {\"from_type\": \"DEPARTMENT\", \"to_type\": \"FACULTY\", \"relationship_type\": \"PART_OF\"},\n",
    "        ('job_entities', 'faculty_entities'): {\"from_type\": \"DEPARTMENT\", \"to_type\": \"FACULTY\", \"relationship_type\": \"PART_OF\"},\n",
    "        ('major_entities', 'degree_entities'): {\"from_type\": \"MAJOR\", \"to_type\": \"DEGREE\", \"relationship_type\": \"IS_UNDER\"},\n",
    "        ('job_entities', 'skill_entities'): {\"from_type\": \"JOB\", \"to_type\": \"SKILL\", \"relationship_type\": \"REQUIRES\"},\n",
    "        ('module_entities', 'skill_entities'): {\"from_type\": \"MODULE\", \"to_type\": \"SKILL\", \"relationship_type\": \"SKILL_TAUGHT\"},\n",
    "        ## ADDED\n",
    "        ('university_entities', 'degree_entities'): {\"from_type\": \"UNIVERSTITY\", \"to_type\": \"DEGREE\", \"relationship_type\": \"OFFERS\"},\n",
    "\n",
    "    }\n",
    "\n",
    "    # Extract entities function \n",
    "    def extract_entities(csv_file_path):\n",
    "        def parse_entity(x, entity_type):\n",
    "            # Handle dictionary strings e.g. \n",
    "            if isinstance(x, str) and x.startswith('{') and x.endswith('}'):\n",
    "                return ast.literal_eval(x)  # Convert to dictionary\n",
    "            # Handle already existing list \n",
    "            elif isinstance(x, list):\n",
    "                # return [str(item).strip() for item in flatten_list(x)]  \n",
    "                return [(str(item).strip(), entity_type) for item in flatten_list(x)]\n",
    "            # Handle list strings\n",
    "            elif isinstance(x, str) and x.startswith('[') and x.endswith(']'):\n",
    "                try:\n",
    "                    parsed_list = ast.literal_eval(x)  # Convert string representation of list to actual list\n",
    "                    # return [str(item).strip() for item in flatten_list(parsed_list)]\n",
    "                    return [(str(item).strip(), entity_type) for item in flatten_list(parsed_list)]\n",
    "                except (ValueError, SyntaxError):\n",
    "                    # return [x.strip()] \n",
    "                    return [(x.strip(), entity_type)]\n",
    "                \n",
    "            # Handle comma-separated strings\n",
    "            # NEED TO REVISE THIS AS SOME VALUES HAVE COMMA IN ITSELF E.G. COLLEGE OF HUMANITIES, ARTS & SOCIAL SCIENCES \n",
    "            elif pd.notna(x):\n",
    "                return [(str(item).strip(), entity_type) for item in str(x).split(',')]\n",
    "                # return [x.strip()]\n",
    "                # return [(x.strip(), entity_type)]\n",
    "            \n",
    "            # Return an empty list for NaN or other invalid entries\n",
    "            return []\n",
    "        \n",
    "        # Helper function to flatten nested lists\n",
    "        def flatten_list(nested_list):\n",
    "            flat_list = []\n",
    "            for i in nested_list:\n",
    "                if isinstance(i, list):\n",
    "                    flat_list.extend(flatten_list(i))\n",
    "                else:\n",
    "                    flat_list.append(i)\n",
    "            return flat_list\n",
    "\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "\n",
    "        # Extract unique skills from the 'Skills' column in a separate CSV\n",
    "        skills_csv_file_path = '../../backend/data/07 - Jobs and relevant skillset (linkedin).csv'\n",
    "        df_skills = pd.read_csv(skills_csv_file_path)\n",
    "        unique_skills = []\n",
    "        \n",
    "        if 'Skills' in df_skills.columns:\n",
    "            # Modified skills parsing\n",
    "            skills_list = []\n",
    "            for skill_text in df_skills[\"Skills\"].dropna():\n",
    "                # Handle the case where skills are comma-separated\n",
    "                if isinstance(skill_text, str):\n",
    "                    # Remove any square brackets if present\n",
    "                    skill_text = skill_text.strip('[]')\n",
    "                    # Split by comma and clean up each skill\n",
    "                    skills = [skill.strip().strip('\"\\'') for skill in skill_text.split(',')]\n",
    "                    skills_list.extend(skills)\n",
    "            \n",
    "            # Clean up skills and remove duplicates\n",
    "            unique_skills = list(set([re.sub(r'\\s\\(.*\\)', '', skill) for skill in skills_list if skill]))\n",
    "\n",
    "            # remove bracketed abbreviations from skills and the space before it\n",
    "            unique_skills = [re.sub(r'\\s\\(.*\\)', '', skill) for skill in unique_skills]\n",
    "\n",
    "            # remove 'Microsoft ' substring before skills\n",
    "            unique_skills = [re.sub(r'Microsoft\\s', '', skill) for skill in unique_skills]\n",
    "\n",
    "        # # Function to extract skills\n",
    "        # def extract_skills(text):\n",
    "        #     if not isinstance(text, str):\n",
    "        #         return []  # Return an empty list if the input is not a valid string\n",
    "            \n",
    "        #     doc = nlp(text)\n",
    "        #     skills = [] \n",
    "\n",
    "        #     # extract skill entities\n",
    "        #     for skill in unique_skills:\n",
    "        #         # create a regex pattern with word boundaries around the skills \n",
    "        #         pattern = r\"\\b\" + re.escape(skill) + r\"\\b\"\n",
    "    \n",
    "        #         # search for the skills in the text (case-insensitive)\n",
    "        #         if re.search(pattern, text, re.IGNORECASE):\n",
    "        #             skills.append(skill)\n",
    "\n",
    "        #     return skills\n",
    "        \n",
    "        # Function to extract skills \n",
    "        def extract_skills(text, threshold=80):\n",
    "            if not isinstance(text, str):\n",
    "                return []\n",
    "            \n",
    "            skills = []\n",
    "\n",
    "            # extract skill entities\n",
    "            for skill in unique_skills:\n",
    "                # create a regex pattern with word boundaries around the skills \n",
    "                pattern = r\"\\b\" + re.escape(skill) + r\"\\b\"\n",
    "\n",
    "                # search for the skills in the text (case-insensitive)\n",
    "                if re.search(pattern, text, re.IGNORECASE):\n",
    "                    skills.append(skill)\n",
    "\n",
    "            # Fuzzy match for entity resolution if no exact matches found\n",
    "            if not skills:\n",
    "                potential_matches = process.extract(text, unique_skills, limit=5, scorer=fuzz.ratio)\n",
    "                skills = [match[0] for match in potential_matches if match[1] >= threshold]\n",
    "            \n",
    "            return list(set(skills))  # Remove duplicates\n",
    "\n",
    "        # Function to extract staff names\n",
    "        def extract_staff_names(text):\n",
    "            if isinstance(text, str):\n",
    "                doc = nlp(text)\n",
    "                staff = []\n",
    "\n",
    "                # Regex pattern to capture staff names with titles like 'Prof', 'Dr', 'Lecturer', 'Tutor'\n",
    "                staff_pattern = re.compile(r'\\b(Prof|Professor|Dr|Lecturer|Tutor|Instructor)\\s*[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)?', re.IGNORECASE)\n",
    "                \n",
    "                for ent in doc.ents:\n",
    "                    match = staff_pattern.search(ent.text)\n",
    "                    if match:\n",
    "                        staff_name = match.group(0)\n",
    "\n",
    "                        # Exclude unwanted phrases that are falsely detected as staff names\n",
    "                        if not any(word in staff_name.lower() for word in ['tutorial', 'attendance', 'assignment', 'participation', 'ratios', 'draft', 'profile']):\n",
    "                            staff.append(staff_name.strip())\n",
    "\n",
    "                # Remove duplicates in staff\n",
    "                return list(set(staff))  \n",
    "            \n",
    "            return [] \n",
    "\n",
    "        # Extract semester entities\n",
    "        semester_cols = ['semester_01', 'semester_02', 'semester_03', 'semester_04']\n",
    "        if all(col in df.columns for col in semester_cols):\n",
    "            df['semester_entities'] = df.apply(lambda row: [(col, 'SEMESTER') for col in semester_cols if row[col] == 1], axis=1)\n",
    "\n",
    "        # Extract entities using appropriate functions \n",
    "        for col in target_cols:\n",
    "            if col in df.columns:\n",
    "                # new_entity_col = new_entity_cols[col]     \n",
    "                new_entity_col, entity_type = new_entity_cols.get(col, (col, 'UNKNOWN'))  # Default entity type as 'UNKNOWN'  \n",
    "\n",
    "                # If the column is description or message, we apply special extraction\n",
    "                if col in ['description']:\n",
    "                    # df['skill_entities'] = df[col].apply(lambda text: extract_skills(text))\n",
    "                    df['skill_entities'] = df[col].apply(lambda text: [(skill, 'SKILL') for skill in extract_skills(text)])\n",
    "\n",
    "                elif col in ['description', 'message']:\n",
    "                    # df['skill_entities'] = df[col].apply(lambda text: extract_skills(text))\n",
    "                    # df['staff_entities'] = df[col].apply(lambda text: extract_staff_names(text))\n",
    "                    df['skill_entities'] = df[col].apply(lambda text: [(skill, 'SKILL') for skill in extract_skills(text)])\n",
    "                    df['staff_entities'] = df[col].apply(lambda text: [(staff, 'STAFF') for staff in extract_staff_names(text)])\n",
    "\n",
    "                else:\n",
    "                    # Create the new column with extracted entities, using the helper function\n",
    "                    # df[new_entity_col] = df[col].apply(parse_entity)\n",
    "                    df[new_entity_col] = df[col].apply(lambda x: parse_entity(x, entity_type))\n",
    "\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    # Extract relationships function \n",
    "    def create_dynamic_relationship(df, from_type, from_id_col, to_type, to_id_col, relationship_type, output_col):\n",
    "        # List to store formatted relationship dictionaries for each row\n",
    "        relationship_column = []\n",
    "\n",
    "        # Iterate through each row of the DataFrame\n",
    "        for _, row in df.iterrows():\n",
    "            # Extract the from_id and to_id values from the specified columns\n",
    "            # from_ids = row[from_id_col] if isinstance(row[from_id_col], list) else [row[from_id_col]]\n",
    "            # to_ids = row[to_id_col] if isinstance(row[to_id_col], list) else [row[to_id_col]]\n",
    "            from_ids = [entity[0] for entity in row[from_id_col] if isinstance(entity, tuple)] if isinstance(row[from_id_col], list) else [row[from_id_col]]\n",
    "            to_ids = [entity[0] for entity in row[to_id_col] if isinstance(entity, tuple)] if isinstance(row[to_id_col], list) else [row[to_id_col]]\n",
    "\n",
    "            # Create a list of dictionaries\n",
    "            relationship_dict = [\n",
    "                {\n",
    "                    \"from_type\": from_type,\n",
    "                    \"from_id\": from_id,\n",
    "                    \"to_type\": to_type,\n",
    "                    \"to_id\": to_id,\n",
    "                    \"type\": relationship_type\n",
    "                }\n",
    "                for from_id in from_ids if pd.notna(from_id)\n",
    "                for to_id in to_ids if pd.notna(to_id)  # Only include non-NaN to_id values\n",
    "            ]\n",
    "\n",
    "            # Append the relationship dictionary or an empty list if no valid to_id found\n",
    "            relationship_column.append(relationship_dict if relationship_dict else [])\n",
    "\n",
    "        # Add the relationships as a new column to the DataFrame\n",
    "        df[output_col] = relationship_column\n",
    "        \n",
    "        # Return the updated DataFrame with the new relationships column\n",
    "        return df\n",
    "    \n",
    "    # Step 1: Extract Entities \n",
    "    df = extract_entities(csv_file_path)\n",
    "\n",
    "    # Step 2: Extract Relationships based on predefined mappings \n",
    "    for key, relationship_info in relationship_mappings.items():\n",
    "        if len(key) == 3:\n",
    "            from_col, to_col, rel_key = key\n",
    "        elif len(key) == 2:\n",
    "            from_col, to_col = key\n",
    "            rel_key = ''  # Set rel_key as an empty string or any default value as needed\n",
    "\n",
    "        if from_col in df.columns and to_col in df.columns:\n",
    "            output_col = f\"{from_col}_to_{to_col}_{relationship_info['relationship_type'].lower()}_relationship\"\n",
    "            df = create_dynamic_relationship(\n",
    "                df,\n",
    "                relationship_info['from_type'],\n",
    "                from_col,\n",
    "                relationship_info['to_type'],\n",
    "                to_col,\n",
    "                relationship_info['relationship_type'],\n",
    "                output_col\n",
    "            )\n",
    "\n",
    "    # Combine all relationship columns into one\n",
    "    relationship_columns = [col for col in df.columns if '_relationship' in col]\n",
    "    if relationship_columns:\n",
    "        df['relationships'] = df[relationship_columns].apply(lambda row: [item for sublist in row if isinstance(sublist, list) for item in sublist], axis=1)\n",
    "        # Drop the individual relationship columns if no longer needed\n",
    "        df = df.drop(columns=relationship_columns)\n",
    "    else: \n",
    "        # Set to a list of empty lists for each row\n",
    "        df['relationships'] = [[] for _ in range(len(df))]  \n",
    "\n",
    "    # Step 3: Output final df\n",
    "    return df\n",
    "\n",
    "\n",
    "# Extract from existing cleaned datasets \n",
    "# csv_file_path = '../../backend/data/00 - mock_student_data.csv'\n",
    "# csv_file_path = '../../backend/data/01 - mock_module_info.csv'\n",
    "# csv_file_path = '../../backend/data/02 - mock_department_list.csv'\n",
    "# csv_file_path = '../../backend/data/03 - mock_staff_info.csv'\n",
    "# csv_file_path = '../../backend/data/04 - mock_module_reviews.csv'\n",
    "# csv_file_path = '../../backend/data/05 - mock_venue_info.csv'\n",
    "# csv_file_path = '../../backend/data/06 - nus_undergraduate_programmes.csv'\n",
    "csv_file_path = '../../backend/data/07 - Jobs and relevant skillset (linkedin).csv'\n",
    "# csv_file_path = '../../backend/data/08 - jobs_and_tech (ONET).csv'\n",
    "# csv_file_path = '../../backend/data/09 - jobs_and_skills (ONET).csv'\n",
    "# csv_file_path = '../../backend/data/10 - Graduate Employment Survey.csv'\n",
    "\n",
    "# Extract Entities and Relationships\n",
    "df = extract_entities_rs(csv_file_path)\n",
    "\n",
    "# Extract base name and append '_extracted'\n",
    "base_name, ext = os.path.splitext(os.path.basename(csv_file_path))\n",
    "new_file_name = f\"{base_name}_extracted{ext}\"\n",
    "\n",
    "# Save the DataFrame to the new file\n",
    "df.to_csv(new_file_name, index=False)\n",
    "\n",
    "# Print a success message with the new file name\n",
    "print(f\"Data saved to: {new_file_name}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy PhraseMatcher (with fuzzywuzzy) q slow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to: 07 - Jobs and relevant skillset (linkedin)_extracted.csv\n",
      "             Job Title                                             Skills  \\\n",
      "0     Academic Advisor  Academic Advising, Higher Education, Student D...   \n",
      "1       Academic Tutor  Tutoring, Teaching, Peer Tutoring, Mathematics...   \n",
      "2  Account Coordinator  Account Management, Account Coordination, Publ...   \n",
      "3     Account Director  Account Management, Client Services, Client Re...   \n",
      "4    Account Executive  Software as a Service (SaaS), Account Manageme...   \n",
      "\n",
      "                                      skill_entities  \\\n",
      "0  [(Academic Advising, SKILL), (Higher Education...   \n",
      "1  [(Tutoring, SKILL), (Teaching, SKILL), (Peer T...   \n",
      "2  [(Account Management, SKILL), (Account Coordin...   \n",
      "3  [(Account Management, SKILL), (Client Services...   \n",
      "4  [(Software as a Service (SaaS), SKILL), (Accou...   \n",
      "\n",
      "                   job_entities  \\\n",
      "0     [(Academic Advisor, JOB)]   \n",
      "1       [(Academic Tutor, JOB)]   \n",
      "2  [(Account Coordinator, JOB)]   \n",
      "3     [(Account Director, JOB)]   \n",
      "4    [(Account Executive, JOB)]   \n",
      "\n",
      "                                       relationships  \n",
      "0  [{'from_type': 'JOB', 'from_id': 'Academic Adv...  \n",
      "1  [{'from_type': 'JOB', 'from_id': 'Academic Tut...  \n",
      "2  [{'from_type': 'JOB', 'from_id': 'Account Coor...  \n",
      "3  [{'from_type': 'JOB', 'from_id': 'Account Dire...  \n",
      "4  [{'from_type': 'JOB', 'from_id': 'Account Exec...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast \n",
    "import re\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import os \n",
    "from fuzzywuzzy import fuzz, process\n",
    "\n",
    "\n",
    "\n",
    "# Load your spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def extract_entities_rs(csv_file_path): \n",
    "\n",
    "    # Predefined entity columns and their corresponding new column names for entity extraction \n",
    "    target_cols = ['Student_Name', 'Faculties', 'Degree', 'Major', 'Module', 'module_code', 'moduleCode', 'Skills', 'Staff', \n",
    "                   'Modules_Completed', 'department', 'faculty', 'prerequisite', 'preclusion', 'Employee Name', \n",
    "                   'Department', 'Modules Taught', 'Title', 'Job Title', 'Tech Skills', 'university',\n",
    "                   'school', 'degree', 'description', 'message']\n",
    "\n",
    "    new_entity_cols = {\n",
    "        'Student_Name': ('student_entities', 'STUDENT'),\n",
    "        'Degree': ('degree_entities', 'DEGREE'),\n",
    "        'degree': ('degree_entities', 'DEGREE'),\n",
    "        'Major': ('major_entities', 'MAJOR'),\n",
    "        'Module': ('module_entities', 'MODULE'),\n",
    "        'Modules_Completed': ('module_entities', 'MODULE'),\n",
    "        'module_code': ('module_entities', 'MODULE'),\n",
    "        'moduleCode': ('module_entities', 'MODULE'),\n",
    "        'Modules Taught': ('module_entities', 'MODULE'),\n",
    "        'prerequisite': ('prerequisite_entities', 'PREREQUISITEGROUP'),\n",
    "        'preclusion': ('preclusion_entities', 'PRECLUSIONGROUP'),\n",
    "        'Skills': ('skill_entities', 'SKILL'),\n",
    "        'Tech Skill': ('skill_entities', 'SKILL'),\n",
    "        'Staff': ('staff_entities', 'STAFF'),\n",
    "        'Employee Name': ('staff_entities', 'STAFF'),\n",
    "        'department': ('department_entities', 'DEPARTMENT'), \n",
    "        'Department': ('department_entities', 'DEPARTMENT'), \n",
    "        'Faculties': ('faculty_entities', 'FACULTY'),\n",
    "        'faculty': ('faculty_entities', 'FACULTY'),\n",
    "        'school': ('faculty_entities', 'FACULTY'),\n",
    "        'Title': ('job_entities', 'JOB'),\n",
    "        'Job Title': ('job_entities', 'JOB'),\n",
    "        'university': ('university_entities', 'UNIVERSITY'),\n",
    "    }\n",
    "\n",
    "    # Relationship mappings\n",
    "    relationship_mappings = {\n",
    "        ('student_entities', 'faculty_entities'): {\"from_type\": \"STUDENT\", \"to_type\": \"FACULTY\", \"relationship_type\": \"STUDYING_UNDER\"},\n",
    "        ('student_entities', 'major_entities'): {\"from_type\": \"STUDENT\", \"to_type\": \"MAJOR\", \"relationship_type\": \"MAJOR_IN\"},\n",
    "        ('student_entities', 'module_entities'): {\"from_type\": \"STUDENT\", \"to_type\": \"MODULE\", \"relationship_type\": \"COMPLETED\"},\n",
    "        ('module_entities', 'department_entities'): {\"from_type\": \"MODULE\", \"to_type\": \"DEPARTMENT\", \"relationship_type\": \"BELONGS_TO\"},\n",
    "        ('module_entities', 'prerequisite_entities', 'MUST_HAVE_TAKEN_ONE_OF'): {\"from_type\": \"MODULE\", \"to_type\": \"DEPARTMENT\", \"relationship_type\": \"MUST_HAVE_TAKEN_ONE_OF\"},\n",
    "        ('module_entities', 'preclusion_entities', 'MUST_NOT_HAVE_TAKEN_ONE_OF'): {\"from_type\": \"MODULE\", \"to_type\": \"DEPARTMENT\", \"relationship_type\": \"MUST_NOT_HAVE_TAKEN_ONE_OF\"},\n",
    "\n",
    "        ('module_entities', 'prerequisite_entities', 'INCLUDED_AS_PREREQUISITE'): {\"from_type\": \"MODULE\", \"to_type\": \"DEPARTMENT\", \"relationship_type\": \"INCLUDED_AS_PREREQUISITE\"},\n",
    "        ('module_entities', 'preclusion_entities', 'INCLUDED_AS_PRECLUSION'): {\"from_type\": \"MODULE\", \"to_type\": \"DEPARTMENT\", \"relationship_type\": \"INCLUDED_AS_PRECLUSION\"},\n",
    "        ('module_entities', 'semester_entities', 'OFFERED_IN'): {\"from_type\": \"MODULE\", \"to_type\": \"SEMESTER\", \"relationship_type\": \"OFFERED_IN\"},\n",
    "        #SEMESTER (can be excluded as this is exception for module info?)\n",
    "        ('module_entities', 'staff_entities'): {\"from_type\": \"MODULE\", \"to_type\": \"STAFF\", \"relationship_type\": \"TAUGHT_BY\"},\n",
    "        ('staff_entities', 'department_entities'): {\"from_type\": \"STAFF\", \"to_type\": \"DEPARTMENT\", \"relationship_type\": \"EMPLOYED_UNDER\"},\n",
    "        ('department_entities', 'faculty_entities'): {\"from_type\": \"DEPARTMENT\", \"to_type\": \"FACULTY\", \"relationship_type\": \"PART_OF\"},\n",
    "        ('job_entities', 'faculty_entities'): {\"from_type\": \"DEPARTMENT\", \"to_type\": \"FACULTY\", \"relationship_type\": \"PART_OF\"},\n",
    "        ('major_entities', 'degree_entities'): {\"from_type\": \"MAJOR\", \"to_type\": \"DEGREE\", \"relationship_type\": \"IS_UNDER\"},\n",
    "        ('job_entities', 'skill_entities'): {\"from_type\": \"JOB\", \"to_type\": \"SKILL\", \"relationship_type\": \"REQUIRES\"},\n",
    "        ('module_entities', 'skill_entities'): {\"from_type\": \"MODULE\", \"to_type\": \"SKILL\", \"relationship_type\": \"SKILL_TAUGHT\"},\n",
    "        ## ADDED\n",
    "        ('university_entities', 'degree_entities'): {\"from_type\": \"UNIVERSTITY\", \"to_type\": \"DEGREE\", \"relationship_type\": \"OFFERS\"},\n",
    "\n",
    "    }\n",
    "\n",
    "    # Extract entities function \n",
    "    def extract_entities(csv_file_path):\n",
    "        def parse_entity(x, entity_type):\n",
    "            # Handle dictionary strings e.g. \n",
    "            if isinstance(x, str) and x.startswith('{') and x.endswith('}'):\n",
    "                return ast.literal_eval(x)  # Convert to dictionary\n",
    "            # Handle already existing list \n",
    "            elif isinstance(x, list):\n",
    "                return [(str(item).strip(), entity_type) for item in flatten_list(x)]\n",
    "            # Handle list strings\n",
    "            elif isinstance(x, str) and x.startswith('[') and x.endswith(']'):\n",
    "                try:\n",
    "                    parsed_list = ast.literal_eval(x)  # Convert string representation of list to actual list\n",
    "                    return [(str(item).strip(), entity_type) for item in flatten_list(parsed_list)]\n",
    "                except (ValueError, SyntaxError):\n",
    "                    # return [x.strip()] \n",
    "                    return [(x.strip(), entity_type)]\n",
    "                \n",
    "            # Handle comma-separated strings\n",
    "            # NEED TO REVISE THIS AS SOME VALUES HAVE COMMA IN ITSELF E.G. COLLEGE OF HUMANITIES, ARTS & SOCIAL SCIENCES \n",
    "            elif pd.notna(x):\n",
    "                return [(str(item).strip(), entity_type) for item in str(x).split(',')]\n",
    "                # return [(x.strip(), entity_type)]\n",
    "            \n",
    "            # Return an empty list for NaN or other invalid entries\n",
    "            return []\n",
    "        \n",
    "        # Helper function to flatten nested lists\n",
    "        def flatten_list(nested_list):\n",
    "            flat_list = []\n",
    "            for i in nested_list:\n",
    "                if isinstance(i, list):\n",
    "                    flat_list.extend(flatten_list(i))\n",
    "                else:\n",
    "                    flat_list.append(i)\n",
    "            return flat_list\n",
    "\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "\n",
    "        # Extract unique skills from the 'Skills' column in a separate CSV\n",
    "        skills_csv_file_path = '../../backend/data/07 - Jobs and relevant skillset (linkedin).csv'\n",
    "        df_skills = pd.read_csv(skills_csv_file_path)\n",
    "        unique_skills = []\n",
    "        \n",
    "        if 'Skills' in df_skills.columns:\n",
    "            # Modified skills parsing\n",
    "            skills_list = []\n",
    "            for skill_text in df_skills[\"Skills\"].dropna():\n",
    "                # Handle the case where skills are comma-separated\n",
    "                if isinstance(skill_text, str):\n",
    "                    # Remove any square brackets if present\n",
    "                    skill_text = skill_text.strip('[]')\n",
    "                    # Split by comma and clean up each skill\n",
    "                    skills = [skill.strip().strip('\"\\'') for skill in skill_text.split(',')]\n",
    "                    skills_list.extend(skills)\n",
    "            \n",
    "            # Clean up skills and remove duplicates\n",
    "            unique_skills = list(set([re.sub(r'\\s\\(.*\\)', '', skill) for skill in skills_list if skill]))\n",
    "\n",
    "            # remove bracketed abbreviations from skills and the space before it\n",
    "            unique_skills = [re.sub(r'\\s\\(.*\\)', '', skill) for skill in unique_skills]\n",
    "\n",
    "            # remove 'Microsoft ' substring before skills\n",
    "            unique_skills = [re.sub(r'Microsoft\\s', '', skill) for skill in unique_skills]\n",
    "        \n",
    "        # Initialize the matcher with the nlp vocabulary\n",
    "        matcher = PhraseMatcher(nlp.vocab)\n",
    "        patterns = [nlp(skill) for skill in unique_skills]  # Assuming unique_skills is defined globally\n",
    "        matcher.add(\"SKILLS\", patterns)\n",
    "\n",
    "\n",
    "        # Function to extract skills using PhraseMatcher\n",
    "        def extract_skills_using_phrasematcher(text, matcher, unique_skills, threshold=80):\n",
    "            if not isinstance(text, str) or not text.strip():\n",
    "                return []  # Return empty list if not a valid non-empty string\n",
    "            doc = nlp(text)\n",
    "            matches = matcher(doc)\n",
    "\n",
    "            # If PhraseMatcher finds matches, use those\n",
    "            if matches:\n",
    "                skills = [doc[start:end].text for match_id, start, end in matches]\n",
    "            else:\n",
    "                # If no exact matches found, fall back on fuzzy matching\n",
    "                skills = []\n",
    "                # Apply fuzzy matching only on tokens with a minimum length of 3 to avoid warnings\n",
    "                tokens = [token.text for token in doc if len(token.text.strip()) > 2]\n",
    "                \n",
    "                # Apply fuzzy matching on each token in the text\n",
    "                for token in tokens:\n",
    "                    if token.strip():\n",
    "                        potential_matches = process.extract(token, unique_skills, limit=1, scorer=fuzz.ratio)\n",
    "                        # Only keep matches above the threshold\n",
    "                        skills.extend([match[0] for match in potential_matches if match[1] >= threshold])\n",
    "            \n",
    "            return list(set(skills))  # Remove duplicates\n",
    "            \n",
    "            # skills = [doc[start:end].text for match_id, start, end in matches]\n",
    "            # return list(set(skills))  # Remove duplicates\n",
    "\n",
    "        # Function to extract staff names\n",
    "        def extract_staff_names(text):\n",
    "            if isinstance(text, str):\n",
    "                doc = nlp(text)\n",
    "                staff = []\n",
    "\n",
    "                # Regex pattern to capture staff names with titles like 'Prof', 'Dr', 'Lecturer', 'Tutor'\n",
    "                staff_pattern = re.compile(r'\\b(Prof|Professor|Dr|Lecturer|Tutor|Instructor)\\s*[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)?', re.IGNORECASE)\n",
    "                \n",
    "                for ent in doc.ents:\n",
    "                    match = staff_pattern.search(ent.text)\n",
    "                    if match:\n",
    "                        staff_name = match.group(0)\n",
    "\n",
    "                        # Exclude unwanted phrases that are falsely detected as staff names\n",
    "                        if not any(word in staff_name.lower() for word in ['tutorial', 'attendance', 'assignment', 'participation', 'ratios', 'draft', 'profile']):\n",
    "                            staff.append(staff_name.strip())\n",
    "\n",
    "                # Remove duplicates in staff\n",
    "                return list(set(staff))  \n",
    "            \n",
    "            return [] \n",
    "\n",
    "        # Extract semester entities\n",
    "        semester_cols = ['semester_01', 'semester_02', 'semester_03', 'semester_04']\n",
    "        if all(col in df.columns for col in semester_cols):\n",
    "            df['semester_entities'] = df.apply(lambda row: [(col, 'SEMESTER') for col in semester_cols if row[col] == 1], axis=1)\n",
    "\n",
    "        # Extract entities using appropriate functions \n",
    "        for col in target_cols:\n",
    "            if col in df.columns:\n",
    "                # new_entity_col = new_entity_cols[col]     \n",
    "                new_entity_col, entity_type = new_entity_cols.get(col, (col, 'UNKNOWN'))  # Default entity type as 'UNKNOWN'  \n",
    "\n",
    "                # If the column is description or message, we apply special extraction\n",
    "                if col in ['description']:\n",
    "                    df['skill_entities'] = df[col].apply(lambda text: [(skill, 'SKILL') for skill in extract_skills_using_phrasematcher(text, matcher, unique_skills)])\n",
    "\n",
    "                elif col in ['description', 'message']:\n",
    "                    df['skill_entities'] = df[col].apply(lambda text: [(skill, 'SKILL') for skill in extract_skills_using_phrasematcher(text, matcher, unique_skills)])\n",
    "                    df['staff_entities'] = df[col].apply(lambda text: [(staff, 'STAFF') for staff in extract_staff_names(text)])\n",
    "\n",
    "                else:\n",
    "                    # Create the new column with extracted entities, using the helper function\n",
    "                    df[new_entity_col] = df[col].apply(lambda x: parse_entity(x, entity_type))\n",
    "\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    # Extract relationships function \n",
    "    def create_dynamic_relationship(df, from_type, from_id_col, to_type, to_id_col, relationship_type, output_col):\n",
    "        # List to store formatted relationship dictionaries for each row\n",
    "        relationship_column = []\n",
    "\n",
    "        # Iterate through each row of the DataFrame\n",
    "        for _, row in df.iterrows():\n",
    "            # Extract the from_id and to_id values from the specified columns\n",
    "            from_ids = [entity[0] for entity in row[from_id_col] if isinstance(entity, tuple)] if isinstance(row[from_id_col], list) else [row[from_id_col]]\n",
    "            to_ids = [entity[0] for entity in row[to_id_col] if isinstance(entity, tuple)] if isinstance(row[to_id_col], list) else [row[to_id_col]]\n",
    "\n",
    "            # Create a list of dictionaries\n",
    "            relationship_dict = [\n",
    "                {\n",
    "                    \"from_type\": from_type,\n",
    "                    \"from_id\": from_id,\n",
    "                    \"to_type\": to_type,\n",
    "                    \"to_id\": to_id,\n",
    "                    \"type\": relationship_type\n",
    "                }\n",
    "                for from_id in from_ids if pd.notna(from_id)\n",
    "                for to_id in to_ids if pd.notna(to_id)  # Only include non-NaN to_id values\n",
    "            ]\n",
    "\n",
    "            # Append the relationship dictionary or an empty list if no valid to_id found\n",
    "            relationship_column.append(relationship_dict if relationship_dict else [])\n",
    "\n",
    "        # Add the relationships as a new column to the DataFrame\n",
    "        df[output_col] = relationship_column\n",
    "        \n",
    "        # Return the updated DataFrame with the new relationships column\n",
    "        return df\n",
    "    \n",
    "    # Step 1: Extract Entities \n",
    "    df = extract_entities(csv_file_path)\n",
    "\n",
    "    # Step 2: Extract Relationships based on predefined mappings \n",
    "    for key, relationship_info in relationship_mappings.items():\n",
    "        if len(key) == 3:\n",
    "            from_col, to_col, rel_key = key\n",
    "        elif len(key) == 2:\n",
    "            from_col, to_col = key\n",
    "            rel_key = ''  # Set rel_key as an empty string or any default value as needed\n",
    "\n",
    "        if from_col in df.columns and to_col in df.columns:\n",
    "            output_col = f\"{from_col}_to_{to_col}_{relationship_info['relationship_type'].lower()}_relationship\"\n",
    "            df = create_dynamic_relationship(\n",
    "                df,\n",
    "                relationship_info['from_type'],\n",
    "                from_col,\n",
    "                relationship_info['to_type'],\n",
    "                to_col,\n",
    "                relationship_info['relationship_type'],\n",
    "                output_col\n",
    "            )\n",
    "\n",
    "    # Combine all relationship columns into one\n",
    "    relationship_columns = [col for col in df.columns if '_relationship' in col]\n",
    "    if relationship_columns:\n",
    "        df['relationships'] = df[relationship_columns].apply(lambda row: [item for sublist in row if isinstance(sublist, list) for item in sublist], axis=1)\n",
    "        # Drop the individual relationship columns if no longer needed\n",
    "        df = df.drop(columns=relationship_columns)\n",
    "    else: \n",
    "        # Set to a list of empty lists for each row\n",
    "        df['relationships'] = [[] for _ in range(len(df))]  \n",
    "\n",
    "    # Step 3: Output final df\n",
    "    return df\n",
    "\n",
    "\n",
    "# Extract from existing cleaned datasets \n",
    "# csv_file_path = '../../backend/data/00 - mock_student_data.csv'\n",
    "# csv_file_path = '../../backend/data/01 - mock_module_info.csv'\n",
    "# csv_file_path = '../../backend/data/02 - mock_department_list.csv'\n",
    "# csv_file_path = '../../backend/data/03 - mock_staff_info.csv'\n",
    "# csv_file_path = '../../backend/data/04 - mock_module_reviews.csv'\n",
    "# csv_file_path = '../../backend/data/05 - mock_venue_info.csv'\n",
    "# csv_file_path = '../../backend/data/06 - nus_undergraduate_programmes.csv'\n",
    "csv_file_path = '../../backend/data/07 - Jobs and relevant skillset (linkedin).csv'\n",
    "# csv_file_path = '../../backend/data/08 - jobs_and_tech (ONET).csv'\n",
    "# csv_file_path = '../../backend/data/09 - jobs_and_skills (ONET).csv'\n",
    "# csv_file_path = '../../backend/data/10 - Graduate Employment Survey.csv'\n",
    "\n",
    "# Extract Entities and Relationships\n",
    "df = extract_entities_rs(csv_file_path)\n",
    "\n",
    "# Extract base name and append '_extracted'\n",
    "base_name, ext = os.path.splitext(os.path.basename(csv_file_path))\n",
    "new_file_name = f\"{base_name}_extracted{ext}\"\n",
    "\n",
    "# Save the DataFrame to the new file\n",
    "df.to_csv(new_file_name, index=False)\n",
    "\n",
    "# Print a success message with the new file name\n",
    "print(f\"Data saved to: {new_file_name}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy (without fuzzywuzzy) --> the fastest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to: 07 - Jobs and relevant skillset (linkedin)_extracted.csv\n",
      "             Job Title                                             Skills  \\\n",
      "0     Academic Advisor  Academic Advising, Higher Education, Student D...   \n",
      "1       Academic Tutor  Tutoring, Teaching, Peer Tutoring, Mathematics...   \n",
      "2  Account Coordinator  Account Management, Account Coordination, Publ...   \n",
      "3     Account Director  Account Management, Client Services, Client Re...   \n",
      "4    Account Executive  Software as a Service (SaaS), Account Manageme...   \n",
      "\n",
      "                                      skill_entities  \\\n",
      "0  [(Academic Advising, SKILL), (Higher Education...   \n",
      "1  [(Tutoring, SKILL), (Teaching, SKILL), (Peer T...   \n",
      "2  [(Account Management, SKILL), (Account Coordin...   \n",
      "3  [(Account Management, SKILL), (Client Services...   \n",
      "4  [(Software as a Service (SaaS), SKILL), (Accou...   \n",
      "\n",
      "                   job_entities  \\\n",
      "0     [(Academic Advisor, JOB)]   \n",
      "1       [(Academic Tutor, JOB)]   \n",
      "2  [(Account Coordinator, JOB)]   \n",
      "3     [(Account Director, JOB)]   \n",
      "4    [(Account Executive, JOB)]   \n",
      "\n",
      "                                       relationships  \n",
      "0  [{'from_type': 'JOB', 'from_id': 'Academic Adv...  \n",
      "1  [{'from_type': 'JOB', 'from_id': 'Academic Tut...  \n",
      "2  [{'from_type': 'JOB', 'from_id': 'Account Coor...  \n",
      "3  [{'from_type': 'JOB', 'from_id': 'Account Dire...  \n",
      "4  [{'from_type': 'JOB', 'from_id': 'Account Exec...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast \n",
    "import re\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import os \n",
    "\n",
    "\n",
    "# Load your spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def extract_entities_rs(csv_file_path): \n",
    "\n",
    "    # Predefined entity columns and their corresponding new column names for entity extraction \n",
    "    target_cols = ['Student_Name', 'Faculties', 'Degree', 'Major', 'Module', 'module_code', 'moduleCode', 'Skills', 'Staff', \n",
    "                   'Modules_Completed', 'department', 'faculty', 'prerequisite', 'preclusion', 'Employee Name', \n",
    "                   'Department', 'Modules Taught', 'Title', 'Job Title', 'Tech Skills', 'university',\n",
    "                   'school', 'degree', 'description', 'message']\n",
    "\n",
    "    new_entity_cols = {\n",
    "        'Student_Name': ('student_entities', 'STUDENT'),\n",
    "        'Degree': ('degree_entities', 'DEGREE'),\n",
    "        'degree': ('degree_entities', 'DEGREE'),\n",
    "        'Major': ('major_entities', 'MAJOR'),\n",
    "        'Module': ('module_entities', 'MODULE'),\n",
    "        'Modules_Completed': ('module_entities', 'MODULE'),\n",
    "        'module_code': ('module_entities', 'MODULE'),\n",
    "        'moduleCode': ('module_entities', 'MODULE'),\n",
    "        'Modules Taught': ('module_entities', 'MODULE'),\n",
    "        'prerequisite': ('prerequisite_entities', 'PREREQUISITEGROUP'),\n",
    "        'preclusion': ('preclusion_entities', 'PRECLUSIONGROUP'),\n",
    "        'Skills': ('skill_entities', 'SKILL'),\n",
    "        'Tech Skill': ('skill_entities', 'SKILL'),\n",
    "        'Staff': ('staff_entities', 'STAFF'),\n",
    "        'Employee Name': ('staff_entities', 'STAFF'),\n",
    "        'department': ('department_entities', 'DEPARTMENT'), \n",
    "        'Department': ('department_entities', 'DEPARTMENT'), \n",
    "        'Faculties': ('faculty_entities', 'FACULTY'),\n",
    "        'faculty': ('faculty_entities', 'FACULTY'),\n",
    "        'school': ('faculty_entities', 'FACULTY'),\n",
    "        'Title': ('job_entities', 'JOB'),\n",
    "        'Job Title': ('job_entities', 'JOB'),\n",
    "        'university': ('university_entities', 'UNIVERSITY'),\n",
    "    }\n",
    "\n",
    "    # Relationship mappings\n",
    "    relationship_mappings = {\n",
    "        ('student_entities', 'faculty_entities'): {\"from_type\": \"STUDENT\", \"to_type\": \"FACULTY\", \"relationship_type\": \"STUDYING_UNDER\"},\n",
    "        ('student_entities', 'major_entities'): {\"from_type\": \"STUDENT\", \"to_type\": \"MAJOR\", \"relationship_type\": \"MAJOR_IN\"},\n",
    "        ('student_entities', 'module_entities'): {\"from_type\": \"STUDENT\", \"to_type\": \"MODULE\", \"relationship_type\": \"COMPLETED\"},\n",
    "        ('module_entities', 'department_entities'): {\"from_type\": \"MODULE\", \"to_type\": \"DEPARTMENT\", \"relationship_type\": \"BELONGS_TO\"},\n",
    "        ('module_entities', 'prerequisite_entities', 'MUST_HAVE_TAKEN_ONE_OF'): {\"from_type\": \"MODULE\", \"to_type\": \"DEPARTMENT\", \"relationship_type\": \"MUST_HAVE_TAKEN_ONE_OF\"},\n",
    "        ('module_entities', 'preclusion_entities', 'MUST_NOT_HAVE_TAKEN_ONE_OF'): {\"from_type\": \"MODULE\", \"to_type\": \"DEPARTMENT\", \"relationship_type\": \"MUST_NOT_HAVE_TAKEN_ONE_OF\"},\n",
    "\n",
    "        ('module_entities', 'prerequisite_entities', 'INCLUDED_AS_PREREQUISITE'): {\"from_type\": \"MODULE\", \"to_type\": \"DEPARTMENT\", \"relationship_type\": \"INCLUDED_AS_PREREQUISITE\"},\n",
    "        ('module_entities', 'preclusion_entities', 'INCLUDED_AS_PRECLUSION'): {\"from_type\": \"MODULE\", \"to_type\": \"DEPARTMENT\", \"relationship_type\": \"INCLUDED_AS_PRECLUSION\"},\n",
    "        ('module_entities', 'semester_entities', 'OFFERED_IN'): {\"from_type\": \"MODULE\", \"to_type\": \"SEMESTER\", \"relationship_type\": \"OFFERED_IN\"},\n",
    "        #SEMESTER (can be excluded as this is exception for module info?)\n",
    "        ('module_entities', 'staff_entities'): {\"from_type\": \"MODULE\", \"to_type\": \"STAFF\", \"relationship_type\": \"TAUGHT_BY\"},\n",
    "        ('staff_entities', 'department_entities'): {\"from_type\": \"STAFF\", \"to_type\": \"DEPARTMENT\", \"relationship_type\": \"EMPLOYED_UNDER\"},\n",
    "        ('department_entities', 'faculty_entities'): {\"from_type\": \"DEPARTMENT\", \"to_type\": \"FACULTY\", \"relationship_type\": \"PART_OF\"},\n",
    "        ('job_entities', 'faculty_entities'): {\"from_type\": \"DEPARTMENT\", \"to_type\": \"FACULTY\", \"relationship_type\": \"PART_OF\"},\n",
    "        ('major_entities', 'degree_entities'): {\"from_type\": \"MAJOR\", \"to_type\": \"DEGREE\", \"relationship_type\": \"IS_UNDER\"},\n",
    "        ('job_entities', 'skill_entities'): {\"from_type\": \"JOB\", \"to_type\": \"SKILL\", \"relationship_type\": \"REQUIRES\"},\n",
    "        ('module_entities', 'skill_entities'): {\"from_type\": \"MODULE\", \"to_type\": \"SKILL\", \"relationship_type\": \"SKILL_TAUGHT\"},\n",
    "        ## ADDED\n",
    "        ('university_entities', 'degree_entities'): {\"from_type\": \"UNIVERSTITY\", \"to_type\": \"DEGREE\", \"relationship_type\": \"OFFERS\"},\n",
    "\n",
    "    }\n",
    "\n",
    "    # Extract entities function \n",
    "    def extract_entities(csv_file_path):\n",
    "        def parse_entity(x, entity_type):\n",
    "            # Handle dictionary strings e.g. \n",
    "            if isinstance(x, str) and x.startswith('{') and x.endswith('}'):\n",
    "                return ast.literal_eval(x)  # Convert to dictionary\n",
    "            # Handle already existing list \n",
    "            elif isinstance(x, list):\n",
    "                return [(str(item).strip(), entity_type) for item in flatten_list(x)]\n",
    "            # Handle list strings\n",
    "            elif isinstance(x, str) and x.startswith('[') and x.endswith(']'):\n",
    "                try:\n",
    "                    parsed_list = ast.literal_eval(x)  # Convert string representation of list to actual list\n",
    "                    return [(str(item).strip(), entity_type) for item in flatten_list(parsed_list)]\n",
    "                except (ValueError, SyntaxError):\n",
    "                    # return [x.strip()] \n",
    "                    return [(x.strip(), entity_type)]\n",
    "                \n",
    "            # Handle comma-separated strings\n",
    "            # NEED TO REVISE THIS AS SOME VALUES HAVE COMMA IN ITSELF E.G. COLLEGE OF HUMANITIES, ARTS & SOCIAL SCIENCES \n",
    "            elif pd.notna(x):\n",
    "                return [(str(item).strip(), entity_type) for item in str(x).split(',')]\n",
    "                # return [(x.strip(), entity_type)]\n",
    "            \n",
    "            # Return an empty list for NaN or other invalid entries\n",
    "            return []\n",
    "        \n",
    "        # Helper function to flatten nested lists\n",
    "        def flatten_list(nested_list):\n",
    "            flat_list = []\n",
    "            for i in nested_list:\n",
    "                if isinstance(i, list):\n",
    "                    flat_list.extend(flatten_list(i))\n",
    "                else:\n",
    "                    flat_list.append(i)\n",
    "            return flat_list\n",
    "\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "\n",
    "        # Extract unique skills from the 'Skills' column in a separate CSV\n",
    "        skills_csv_file_path = '../../backend/data/07 - Jobs and relevant skillset (linkedin).csv'\n",
    "        df_skills = pd.read_csv(skills_csv_file_path)\n",
    "        unique_skills = []\n",
    "        \n",
    "        if 'Skills' in df_skills.columns:\n",
    "            # Modified skills parsing\n",
    "            skills_list = []\n",
    "            for skill_text in df_skills[\"Skills\"].dropna():\n",
    "                # Handle the case where skills are comma-separated\n",
    "                if isinstance(skill_text, str):\n",
    "                    # Remove any square brackets if present\n",
    "                    skill_text = skill_text.strip('[]')\n",
    "                    # Split by comma and clean up each skill\n",
    "                    skills = [skill.strip().strip('\"\\'') for skill in skill_text.split(',')]\n",
    "                    skills_list.extend(skills)\n",
    "            \n",
    "            # Clean up skills and remove duplicates\n",
    "            unique_skills = list(set([re.sub(r'\\s\\(.*\\)', '', skill) for skill in skills_list if skill]))\n",
    "\n",
    "            # remove bracketed abbreviations from skills and the space before it\n",
    "            unique_skills = [re.sub(r'\\s\\(.*\\)', '', skill) for skill in unique_skills]\n",
    "\n",
    "            # remove 'Microsoft ' substring before skills\n",
    "            unique_skills = [re.sub(r'Microsoft\\s', '', skill) for skill in unique_skills]\n",
    "        \n",
    "        # Initialize the matcher with the nlp vocabulary\n",
    "        matcher = PhraseMatcher(nlp.vocab)\n",
    "        patterns = [nlp(skill) for skill in unique_skills]  # Assuming unique_skills is defined globally\n",
    "        matcher.add(\"SKILLS\", patterns)\n",
    "\n",
    "\n",
    "        # Function to extract skills using PhraseMatcher\n",
    "        def extract_skills_using_phrasematcher(text, matcher, unique_skills, threshold=80):\n",
    "            if not isinstance(text, str) or not text.strip():\n",
    "                return []  # Return empty list if not a valid non-empty string\n",
    "            doc = nlp(text)\n",
    "            matches = matcher(doc)\n",
    "            skills = [doc[start:end].text for match_id, start, end in matches]\n",
    "            return list(set(skills))  # Remove duplicates\n",
    "\n",
    "        # Function to extract staff names\n",
    "        def extract_staff_names(text):\n",
    "            if isinstance(text, str):\n",
    "                doc = nlp(text)\n",
    "                staff = []\n",
    "\n",
    "                # Regex pattern to capture staff names with titles like 'Prof', 'Dr', 'Lecturer', 'Tutor'\n",
    "                staff_pattern = re.compile(r'\\b(Prof|Professor|Dr|Lecturer|Tutor|Instructor)\\s*[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)?', re.IGNORECASE)\n",
    "                \n",
    "                for ent in doc.ents:\n",
    "                    match = staff_pattern.search(ent.text)\n",
    "                    if match:\n",
    "                        staff_name = match.group(0)\n",
    "\n",
    "                        # Exclude unwanted phrases that are falsely detected as staff names\n",
    "                        if not any(word in staff_name.lower() for word in ['tutorial', 'attendance', 'assignment', 'participation', 'ratios', 'draft', 'profile']):\n",
    "                            staff.append(staff_name.strip())\n",
    "\n",
    "                # Remove duplicates in staff\n",
    "                return list(set(staff))  \n",
    "            \n",
    "            return [] \n",
    "\n",
    "        # Extract semester entities\n",
    "        semester_cols = ['semester_01', 'semester_02', 'semester_03', 'semester_04']\n",
    "        if all(col in df.columns for col in semester_cols):\n",
    "            df['semester_entities'] = df.apply(lambda row: [(col, 'SEMESTER') for col in semester_cols if row[col] == 1], axis=1)\n",
    "\n",
    "        # Extract entities using appropriate functions \n",
    "        for col in target_cols:\n",
    "            if col in df.columns:\n",
    "                # new_entity_col = new_entity_cols[col]     \n",
    "                new_entity_col, entity_type = new_entity_cols.get(col, (col, 'UNKNOWN'))  # Default entity type as 'UNKNOWN'  \n",
    "\n",
    "                # If the column is description or message, we apply special extraction\n",
    "                if col in ['description']:\n",
    "                    df['skill_entities'] = df[col].apply(lambda text: [(skill, 'SKILL') for skill in extract_skills_using_phrasematcher(text, matcher, unique_skills)])\n",
    "\n",
    "                elif col in ['description', 'message']:\n",
    "                    df['skill_entities'] = df[col].apply(lambda text: [(skill, 'SKILL') for skill in extract_skills_using_phrasematcher(text, matcher, unique_skills)])\n",
    "                    df['staff_entities'] = df[col].apply(lambda text: [(staff, 'STAFF') for staff in extract_staff_names(text)])\n",
    "\n",
    "                else:\n",
    "                    # Create the new column with extracted entities, using the helper function\n",
    "                    df[new_entity_col] = df[col].apply(lambda x: parse_entity(x, entity_type))\n",
    "\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    # Extract relationships function \n",
    "    def create_dynamic_relationship(df, from_type, from_id_col, to_type, to_id_col, relationship_type, output_col):\n",
    "        # List to store formatted relationship dictionaries for each row\n",
    "        relationship_column = []\n",
    "\n",
    "        # Iterate through each row of the DataFrame\n",
    "        for _, row in df.iterrows():\n",
    "            # Extract the from_id and to_id values from the specified columns\n",
    "            from_ids = [entity[0] for entity in row[from_id_col] if isinstance(entity, tuple)] if isinstance(row[from_id_col], list) else [row[from_id_col]]\n",
    "            to_ids = [entity[0] for entity in row[to_id_col] if isinstance(entity, tuple)] if isinstance(row[to_id_col], list) else [row[to_id_col]]\n",
    "\n",
    "            # Create a list of dictionaries\n",
    "            relationship_dict = [\n",
    "                {\n",
    "                    \"from_type\": from_type,\n",
    "                    \"from_id\": from_id,\n",
    "                    \"to_type\": to_type,\n",
    "                    \"to_id\": to_id,\n",
    "                    \"type\": relationship_type\n",
    "                }\n",
    "                for from_id in from_ids if pd.notna(from_id)\n",
    "                for to_id in to_ids if pd.notna(to_id)  # Only include non-NaN to_id values\n",
    "            ]\n",
    "\n",
    "            # Append the relationship dictionary or an empty list if no valid to_id found\n",
    "            relationship_column.append(relationship_dict if relationship_dict else [])\n",
    "\n",
    "        # Add the relationships as a new column to the DataFrame\n",
    "        df[output_col] = relationship_column\n",
    "        \n",
    "        # Return the updated DataFrame with the new relationships column\n",
    "        return df\n",
    "    \n",
    "    # Step 1: Extract Entities \n",
    "    df = extract_entities(csv_file_path)\n",
    "\n",
    "    # Step 2: Extract Relationships based on predefined mappings \n",
    "    for key, relationship_info in relationship_mappings.items():\n",
    "        if len(key) == 3:\n",
    "            from_col, to_col, rel_key = key\n",
    "        elif len(key) == 2:\n",
    "            from_col, to_col = key\n",
    "            rel_key = ''  # Set rel_key as an empty string or any default value as needed\n",
    "\n",
    "        if from_col in df.columns and to_col in df.columns:\n",
    "            output_col = f\"{from_col}_to_{to_col}_{relationship_info['relationship_type'].lower()}_relationship\"\n",
    "            df = create_dynamic_relationship(\n",
    "                df,\n",
    "                relationship_info['from_type'],\n",
    "                from_col,\n",
    "                relationship_info['to_type'],\n",
    "                to_col,\n",
    "                relationship_info['relationship_type'],\n",
    "                output_col\n",
    "            )\n",
    "\n",
    "    # Combine all relationship columns into one\n",
    "    relationship_columns = [col for col in df.columns if '_relationship' in col]\n",
    "    if relationship_columns:\n",
    "        df['relationships'] = df[relationship_columns].apply(lambda row: [item for sublist in row if isinstance(sublist, list) for item in sublist], axis=1)\n",
    "        # Drop the individual relationship columns if no longer needed\n",
    "        df = df.drop(columns=relationship_columns)\n",
    "    else: \n",
    "        # Set to a list of empty lists for each row\n",
    "        df['relationships'] = [[] for _ in range(len(df))]  \n",
    "\n",
    "    # Step 3: Output final df\n",
    "    return df\n",
    "\n",
    "\n",
    "# Extract from existing cleaned datasets \n",
    "# csv_file_path = '../../backend/data/00 - mock_student_data.csv'\n",
    "# csv_file_path = '../../backend/data/01 - mock_module_info.csv'\n",
    "# csv_file_path = '../../backend/data/02 - mock_department_list.csv'\n",
    "# csv_file_path = '../../backend/data/03 - mock_staff_info.csv'\n",
    "# csv_file_path = '../../backend/data/04 - mock_module_reviews.csv'\n",
    "# csv_file_path = '../../backend/data/05 - mock_venue_info.csv'\n",
    "# csv_file_path = '../../backend/data/06 - nus_undergraduate_programmes.csv'\n",
    "csv_file_path = '../../backend/data/07 - Jobs and relevant skillset (linkedin).csv'\n",
    "# csv_file_path = '../../backend/data/08 - jobs_and_tech (ONET).csv'\n",
    "# csv_file_path = '../../backend/data/09 - jobs_and_skills (ONET).csv'\n",
    "# csv_file_path = '../../backend/data/10 - Graduate Employment Survey.csv'\n",
    "\n",
    "# Extract Entities and Relationships\n",
    "df = extract_entities_rs(csv_file_path)\n",
    "\n",
    "# Extract base name and append '_extracted'\n",
    "base_name, ext = os.path.splitext(os.path.basename(csv_file_path))\n",
    "new_file_name = f\"{base_name}_extracted{ext}\"\n",
    "\n",
    "# Save the DataFrame to the new file\n",
    "df.to_csv(new_file_name, index=False)\n",
    "\n",
    "# Print a success message with the new file name\n",
    "print(f\"Data saved to: {new_file_name}\")\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
